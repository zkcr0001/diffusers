{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9d75cf-ae21-4bec-a422-10ee67f3088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from diffusers import CycleDiffusionPipeline, DDIMScheduler\n",
    "\n",
    "# load the pipeline\n",
    "# make sure you're logged in with `huggingface-cli login`\n",
    "model_id_or_path = \"CompVis/stable-diffusion-v1-4\"\n",
    "scheduler = DDIMScheduler.from_pretrained(model_id_or_path, subfolder=\"scheduler\")\n",
    "pipe = CycleDiffusionPipeline.from_pretrained(model_id_or_path, scheduler=scheduler).to(\"cuda\",torch.float64)\n",
    "\n",
    "# let's download an initial image\n",
    "url = \"https://raw.githubusercontent.com/ChenWu98/cycle-diffusion/main/data/dalle2/An%20astronaut%20riding%20a%20horse.png\"\n",
    "response = requests.get(url)\n",
    "init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "init_image = init_image.resize((512, 512))\n",
    "init_image.save(\"horse.png\")\n",
    "plt.imshow(init_image)\n",
    "plt.show()\n",
    "\n",
    "# let's specify a prompt\n",
    "source_prompt = \"An astronaut riding a horse\"\n",
    "prompt = \"An astronaut riding an elephant\"\n",
    "\n",
    "# call the pipeline\n",
    "image1 = pipe(\n",
    "    prompt=prompt,\n",
    "    source_prompt=source_prompt,\n",
    "    image=init_image,\n",
    "    num_inference_steps=100,\n",
    "    eta=0.1,\n",
    "    strength=0.8,\n",
    "    guidance_scale=2,\n",
    "    source_guidance_scale=1,\n",
    ").images[0]\n",
    "plt.imshow(image1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb521f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    if not isinstance(imgs[0], np.ndarray):\n",
    "        w, h = imgs[0].size\n",
    "        grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "        grid_w, grid_h = grid.size\n",
    "        \n",
    "        for i, img in enumerate(imgs):\n",
    "            grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    else:\n",
    "        w, h = Image.fromarray((imgs[0] * 255).astype(np.uint8)).size\n",
    "        grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "        grid_w, grid_h = grid.size\n",
    "        \n",
    "        for i, img in enumerate(imgs):\n",
    "            grid.paste(Image.fromarray((img * 255).astype(np.uint8)), box=(i%cols*w, i//cols*h))   \n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33b2611-15d9-42e8-9c46-3ff971ff9938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# load the pipeline\n",
    "# make sure you're logged in with `huggingface-cli login`\n",
    "model_id_or_path = \"gsdf/Counterfeit-V2.5\"\n",
    "scheduler = DDIMScheduler.from_pretrained(model_id_or_path, subfolder=\"scheduler\")\n",
    "pipe = CycleDiffusionPipeline.from_pretrained(model_id_or_path, scheduler=scheduler).to(\"cuda\",torch.float64)\n",
    "\n",
    "# let's download an initial image\n",
    "init_image = Image.open(\"/home/ubuntu/anime_dance/0001.png\").convert(\"RGB\")\n",
    "init_image = init_image.resize((512, 512))\n",
    "\n",
    "torch.manual_seed(30)\n",
    "# let's specify a prompt\n",
    "source_prompt = \"masterpiece,best quality, 1girl, long hair, black hair, solo, black dress, white tie, white scarf, looking at viewer\"\n",
    "prompt = \"masterpiece,best quality, 1girl, long hair, red hair, solo, black dress, white tie, white scarf, looking at viewer\"\n",
    "\n",
    "# call the pipeline\n",
    "image1 = pipe(\n",
    "    prompt=prompt,\n",
    "    source_prompt=source_prompt,\n",
    "    image=init_image,\n",
    "    num_inference_steps=80,\n",
    "    eta=0.1,\n",
    "    strength=0.8,\n",
    "    guidance_scale=7.5,\n",
    "    source_guidance_scale=7.5,\n",
    ").images[0]\n",
    "\n",
    "grid = image_grid([init_image, image1], rows=1, cols=2)\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d089ab14",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cycle Diffusion Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efd4cbc-4a43-45b2-9689-c7241a916e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "def preprocess(image):\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        return image\n",
    "    elif isinstance(image, PIL.Image.Image):\n",
    "        image = [image]\n",
    "\n",
    "    if isinstance(image[0], PIL.Image.Image):\n",
    "        w, h = image[0].size\n",
    "        w, h = (x - x % 8 for x in (w, h))  # resize to integer multiple of 8\n",
    "\n",
    "        image = [np.array(i.resize((w, h), resample=PIL.Image.LANCZOS))[None, :] for i in image]\n",
    "        image = np.concatenate(image, axis=0)\n",
    "        image = np.array(image).astype(np.float32) / 255.0\n",
    "        image = image.transpose(0, 3, 1, 2)\n",
    "        image = 2.0 * image - 1.0\n",
    "        image = torch.from_numpy(image)\n",
    "    elif isinstance(image[0], torch.Tensor):\n",
    "        image = torch.cat(image, dim=0)\n",
    "    return image\n",
    "\n",
    "def posterior_sample(scheduler, latents, timestep, clean_latents, generator, eta):\n",
    "    # 1. get previous step value (=t-1)\n",
    "    prev_timestep = timestep - scheduler.config.num_train_timesteps // scheduler.num_inference_steps\n",
    "\n",
    "    if prev_timestep <= 0:\n",
    "        return clean_latents\n",
    "\n",
    "    # 2. compute alphas, betas\n",
    "    alpha_prod_t = scheduler.alphas_cumprod[timestep]\n",
    "    alpha_prod_t_prev = (\n",
    "        scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else scheduler.final_alpha_cumprod\n",
    "    )\n",
    "\n",
    "    variance = scheduler._get_variance(timestep, prev_timestep)\n",
    "    std_dev_t = eta * variance ** (0.5)\n",
    "\n",
    "    # direction pointing to x_t\n",
    "    e_t = (latents - alpha_prod_t ** (0.5) * clean_latents) / (1 - alpha_prod_t) ** (0.5)\n",
    "    dir_xt = (1.0 - alpha_prod_t_prev - std_dev_t**2) ** (0.5) * e_t\n",
    "    noise = std_dev_t * randn_tensor(\n",
    "        clean_latents.shape, dtype=clean_latents.dtype, device=clean_latents.device, generator=generator\n",
    "    )\n",
    "    prev_latents = alpha_prod_t_prev ** (0.5) * clean_latents + dir_xt + noise\n",
    "\n",
    "    return prev_latents\n",
    "\n",
    "\n",
    "def compute_noise(scheduler, prev_latents, latents, timestep, noise_pred, eta):\n",
    "    # 1. get previous step value (=t-1)\n",
    "    prev_timestep = timestep - scheduler.config.num_train_timesteps // scheduler.num_inference_steps\n",
    "\n",
    "    # 2. compute alphas, betas\n",
    "    alpha_prod_t = scheduler.alphas_cumprod[timestep]\n",
    "    alpha_prod_t_prev = (\n",
    "        scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else scheduler.final_alpha_cumprod\n",
    "    )\n",
    "\n",
    "    beta_prod_t = 1 - alpha_prod_t\n",
    "\n",
    "    # 3. compute predicted original sample from predicted noise also called\n",
    "    # \"predicted x_0\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
    "    pred_original_sample = (latents - beta_prod_t ** (0.5) * noise_pred) / alpha_prod_t ** (0.5)\n",
    "\n",
    "    # 4. Clip \"predicted x_0\"\n",
    "    if scheduler.config.clip_sample:\n",
    "        pred_original_sample = torch.clamp(pred_original_sample, -1, 1)\n",
    "\n",
    "    # 5. compute variance: \"sigma_t(η)\" -> see formula (16)\n",
    "    # σ_t = sqrt((1 − α_t−1)/(1 − α_t)) * sqrt(1 − α_t/α_t−1)\n",
    "    variance = scheduler._get_variance(timestep, prev_timestep)\n",
    "    std_dev_t = eta * variance ** (0.5)\n",
    "\n",
    "    # 6. compute \"direction pointing to x_t\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
    "    pred_sample_direction = (1 - alpha_prod_t_prev - std_dev_t**2) ** (0.5) * noise_pred\n",
    "\n",
    "    noise = (prev_latents - (alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction)) / (\n",
    "        variance ** (0.5) * eta\n",
    "    )\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ddd478-1486-41ab-bc69-897bcddf698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_prompt = \"masterpiece,best quality, 1girl, long hair, red hair, solo, dress, blue eyes, looking at viewer\"\n",
    "prompt = \"masterpiece,best quality, 1girl, long hair, red hair, solo, dress, red eyes, looking at viewer\"\n",
    "init_image = Image.open(\"/home/ubuntu/anime_dance/0001.png\").convert(\"RGB\")\n",
    "init_image = init_image.resize((512, 512))\n",
    "strength = 0.8\n",
    "num_inference_steps = 50\n",
    "guidance_scale = 7.5\n",
    "source_guidance_scale = 7.5\n",
    "num_images_per_prompt = 1\n",
    "eta = 0.1\n",
    "generator = None\n",
    "prompt_embeds = None\n",
    "output_type = \"pil\"\n",
    "return_dict = True\n",
    "callback = None\n",
    "callback_steps = 1\n",
    "\n",
    "\n",
    "scheduler = DDIMScheduler.from_pretrained(model_id_or_path, subfolder=\"scheduler\")\n",
    "pipe.scheduler = scheduler\n",
    "\n",
    "torch.manual_seed(11)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # 1. Check inputs\n",
    "    pipe.check_inputs(prompt, strength, callback_steps)\n",
    "\n",
    "    # 2. Define call parameters\n",
    "    batch_size = 1\n",
    "    device = pipe._execution_device\n",
    "    do_classifier_free_guidance = guidance_scale > 1.0\n",
    "\n",
    "    # 3. Encode input prompt\n",
    "    prompt_embeds = pipe._encode_prompt(\n",
    "                prompt,\n",
    "                device,\n",
    "                num_images_per_prompt,\n",
    "                do_classifier_free_guidance,\n",
    "                prompt_embeds=prompt_embeds,\n",
    "            )\n",
    "    source_prompt_embeds = pipe._encode_prompt(\n",
    "        source_prompt, device, num_images_per_prompt, do_classifier_free_guidance, None\n",
    "    )\n",
    "    print(source_prompt_embeds.shape)\n",
    "\n",
    "    # 4. Preprocess image\n",
    "    image = preprocess(init_image)\n",
    "    print(image.shape)\n",
    "    print(torch.max(image), torch.min(image))\n",
    "\n",
    "    # 5. Prepare timesteps\n",
    "    pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "    timesteps, num_inference_steps = pipe.get_timesteps(num_inference_steps, strength, device)\n",
    "    latent_timestep = timesteps[:1].repeat(batch_size * num_images_per_prompt)\n",
    "    print(latent_timestep)\n",
    "\n",
    "    # 6. Prepare latent variables\n",
    "    latents, clean_latents = pipe.prepare_latents(\n",
    "        image, latent_timestep, batch_size, num_images_per_prompt, prompt_embeds.dtype, device, generator\n",
    "    )\n",
    "    source_latents = latents\n",
    "    print(\"latent shape:\", latents.shape, clean_latents.shape)\n",
    "\n",
    "    # 7. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n",
    "    extra_step_kwargs = pipe.prepare_extra_step_kwargs(generator, eta)\n",
    "    generator = extra_step_kwargs.pop(\"generator\", None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e72c635-6d53-48a0-9dfa-42d501f33dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Denoising loop\n",
    "from diffusers.utils import randn_tensor\n",
    "\n",
    "with torch.no_grad():\n",
    "    num_warmup_steps = len(timesteps) - num_inference_steps * pipe.scheduler.order\n",
    "    with pipe.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "        for i, t in enumerate(timesteps):\n",
    "            # expand the latents if we are doing classifier free guidance\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "            source_latent_model_input = torch.cat([source_latents] * 2)\n",
    "            latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)\n",
    "            source_latent_model_input = pipe.scheduler.scale_model_input(source_latent_model_input, t)\n",
    "\n",
    "            # predict the noise residual\n",
    "            concat_latent_model_input = torch.stack(\n",
    "                [\n",
    "                    source_latent_model_input[0],\n",
    "                    latent_model_input[0],\n",
    "                    source_latent_model_input[1],\n",
    "                    latent_model_input[1],\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "            concat_prompt_embeds = torch.stack(\n",
    "                [\n",
    "                    source_prompt_embeds[0],\n",
    "                    prompt_embeds[0],\n",
    "                    source_prompt_embeds[1],\n",
    "                    prompt_embeds[1],\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "            concat_noise_pred = pipe.unet(\n",
    "                concat_latent_model_input, t, encoder_hidden_states=concat_prompt_embeds\n",
    "            ).sample\n",
    "\n",
    "            # perform guidance\n",
    "            (\n",
    "                source_noise_pred_uncond,\n",
    "                noise_pred_uncond,\n",
    "                source_noise_pred_text,\n",
    "                noise_pred_text,\n",
    "            ) = concat_noise_pred.chunk(4, dim=0)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "            source_noise_pred = source_noise_pred_uncond + source_guidance_scale * (\n",
    "                source_noise_pred_text - source_noise_pred_uncond\n",
    "            )\n",
    "\n",
    "            # Sample source_latents from the posterior distribution.\n",
    "            prev_source_latents = posterior_sample(\n",
    "                pipe.scheduler, source_latents, t, clean_latents, generator=generator, **extra_step_kwargs\n",
    "            )\n",
    "            # Compute noise.\n",
    "            noise = compute_noise(\n",
    "                pipe.scheduler, prev_source_latents, source_latents, t, source_noise_pred, **extra_step_kwargs\n",
    "            )\n",
    "            source_latents = prev_source_latents\n",
    "\n",
    "            # compute the previous noisy sample x_t -> x_t-1\n",
    "            latents = pipe.scheduler.step(\n",
    "                noise_pred, t, latents, variance_noise=noise, **extra_step_kwargs\n",
    "            ).prev_sample\n",
    "\n",
    "            # call the callback, if provided\n",
    "            if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % pipe.scheduler.order == 0):\n",
    "                progress_bar.update()\n",
    "                if callback is not None and i % callback_steps == 0:\n",
    "                    callback(i, t, latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f105b0cc-5c1d-4de9-9196-45da04949af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # 9. Post-processing\n",
    "    image = pipe.decode_latents(latents)\n",
    "\n",
    "    # 11. Convert to PIL\n",
    "    if output_type == \"pil\":\n",
    "        image = pipe.numpy_to_pil(image)\n",
    "grid = image_grid([init_image, image[0]], rows=1, cols=2)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1adfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # 9. Post-processing\n",
    "    image = pipe.decode_latents(clean_latents)\n",
    "\n",
    "    # 11. Convert to PIL\n",
    "    if output_type == \"pil\":\n",
    "        image = pipe.numpy_to_pil(image)\n",
    "grid = image_grid([init_image, image[0]], rows=1, cols=2)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f61fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97378d97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21e6a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "caf1c2fcf97217de91eafa76b907d50f9ea378f5ffbee7f571142d119bb6a771"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
